{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-telfeyan/toolbox/blob/master/language_ai/OpenAI_FineTune_Completion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Synopsis**: Create a fine-tuned model for OpenAI and use it for prompt completion, text classification, entity extraction, and/or summarization\n",
        "\n",
        "**Created**:  July 2023\n",
        "\n",
        "**Author**:   [John Telfeyan](https://mailhide.io/e/mMkX3)\n",
        "\n",
        "**Distribution**: [MIT Opens Source Copyright](https://gist.github.com/john-telfeyan/2565b2904355410c1e75f27524aeea5f#file-license-md)\n",
        "\n",
        "**Sources**:  \n",
        "https://github.com/Kirili4ik/ruDialoGpt3-finetune-colab  \n",
        "https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb\n",
        "https://stackoverflow.com/questions/75774873/openai-chatgpt-gpt-3-5-api-error-this-is-a-chat-model-and-not-supported-in-t"
      ],
      "metadata": {
        "id": "s6dBxtC_y8M5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preperations\n",
        "dependancies and connection to google drive\n"
      ],
      "metadata": {
        "id": "-_n1e95Sy8Jg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esbu3YcXLbw5",
        "outputId": "3874b85b-ebb0-4261-9b8f-cd434f9879ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "gxk_yol0NxLC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63FwfqoHMrjY",
        "outputId": "17c2ac04-1d31-4e06-df29-2fc2e2a651a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add your API key for OpenAI\n",
        "get one first here:  \n",
        " https://platform.openai.com/account/api-keys\n"
      ],
      "metadata": {
        "id": "ro7p0Bd5yDRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best practice to store your key in a git-ignored file\n",
        "# To-do: use secure string\n",
        "key_file=\"/content/drive/MyDrive/secrets/fine-tuning.openai.key.txt\"\n",
        "\n",
        "with open(key_file, 'r') as f:\n",
        "  key = f.read()\n",
        "\n",
        "# Set an os envrion variable so !bang commands will work\n",
        "os.environ['OPENAI_API_KEY'] = key\n",
        "\n",
        "# Set an object so python commands will work\n",
        "openai.api_key = key\n",
        "#openai.api_key_path = key_file # alternatively read from a file if only python\n",
        "\n",
        "# Key should be about 50 chars\n",
        "print(len(openai.api_key))"
      ],
      "metadata": {
        "id": "vSSmihWhMyeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91de684-af2e-4748-ba80-ee2517b2bf8b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Just want to use ChatGTP without fine-tuning?\n",
        "\n",
        "Chat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, itâ€™s just as useful for single-turn tasks without any conversation.  \n",
        "\n",
        "The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.  You can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation\n",
        "\n",
        "In this example we tell the bot to act as a text classifier and the output we are looking for is \"other\", which we obtain in\n",
        "```bash\n",
        "<OpenAIObject chat.completion>\n",
        "choices -> message -> content `  \n",
        "```\n",
        "Read more:  \n",
        "https://platform.openai.com/docs/guides/gpt/chat-completions-api"
      ],
      "metadata": {
        "id": "7jybKKw60Ahw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a text classification bot that responds with one-word answers.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Classify this text as related to a 'broken' item, 'training' exercise, or 'other': 'The wing broke off my drone.'\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Broken\"},\n",
        "        {\"role\": \"user\", \"content\": \"The bird flew sucessfuly\"}]\n",
        ")\n"
      ],
      "metadata": {
        "id": "TeTujgEyMKXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f01499-41a1-4085-f921-678f8d9b9060"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7bFCO1rdMzCoFoPYKKR1Lt2NuIlVz at 0x7f4b7eafc540> JSON: {\n",
              "  \"id\": \"chatcmpl-7bFCO1rdMzCoFoPYKKR1Lt2NuIlVz\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1689110916,\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": \"Other\"\n",
              "      },\n",
              "      \"finish_reason\": \"stop\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 69,\n",
              "    \"completion_tokens\": 1,\n",
              "    \"total_tokens\": 70\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tuning Example\n",
        "Now lets fine tune a LoRA to do this (even better?) based on some training data that we had ChatGTP generate and then manually fixed."
      ],
      "metadata": {
        "id": "pfzKcRcC3v7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/proj/chatgpt-fine-tune/training-data-source/broken_items_03.csv\")"
      ],
      "metadata": {
        "id": "vQY76DuiX9h7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex_df = df[[\"Text\",\"Broken\"]] #extract a subset df\n",
        "ex_df.columns=[\"prompt\", \"completion\"] # these column names are manditory\n",
        "ex_df.to_json(\"broken_03.jsonl\", orient='records', lines=True) #manditory format"
      ],
      "metadata": {
        "id": "NsZxvbmDYCFx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai tools fine_tunes.prepare_data -f broken_03.jsonl -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHkPjMgw15a2",
        "outputId": "bdaf4d3d-59ed-4112-f078-3f48dccde8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 105 prompt-completion pairs\n",
            "- The `prompt` column/key should be lowercase\n",
            "- The `completion` column/key should be lowercase\n",
            "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
            "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
            "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
            "- There are 18 duplicated prompt-completion sets. These are rows: [39, 41, 42, 43, 44, 46, 47, 49, 50, 51, 53, 55, 57, 58, 60, 61, 63, 64]\n",
            "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Necessary] Lower case column name to `prompt`\n",
            "- [Necessary] Lower case column name to `completion`\n",
            "- [Recommended] Remove 18 duplicate rows [Y/n]: Y\n",
            "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
            "/usr/local/lib/python3.10/dist-packages/openai/validators.py:226: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x[\"prompt\"] += suffix\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
            "/usr/local/lib/python3.10/dist-packages/openai/validators.py:425: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x[\"completion\"] = x[\"completion\"].apply(\n",
            "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified files to `broken_03_prepared_train.jsonl` and `broken_03_prepared_valid.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"broken_03_prepared_train.jsonl\" -v \"broken_03_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 3\n",
            "\n",
            "After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt.\n",
            "Once your model starts training, it'll approximately take 4.42 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t \"broken_03_prepared_train.jsonl\" -v \"broken_03_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9eJcCSD25O7",
        "outputId": "02a13e9d-71c1-475b-b7a2-a4e171837722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rUpload progress:   0% 0.00/12.0k [00:00<?, ?it/s]\rUpload progress: 100% 12.0k/12.0k [00:00<00:00, 12.6Mit/s]\n",
            "Uploaded file from broken_03_prepared_train.jsonl: file-JRhZtUbvSShVYWCS9tCYrwkq\n",
            "Upload progress: 100% 3.10k/3.10k [00:00<00:00, 5.01Mit/s]\n",
            "Uploaded file from broken_03_prepared_valid.jsonl: file-xjqa9VtGTnRGUu0zu77LhS3k\n",
            "Created fine-tune: ft-FWcyueisQGBddHjFgZMBtr5C\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-07-10 03:32:46] Created fine-tune: ft-FWcyueisQGBddHjFgZMBtr5C\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-FWcyueisQGBddHjFgZMBtr5C\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.follow -i ft-FWcyueisQGBddHjFgZMBtr5C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11EUlWsB5lRY",
        "outputId": "50018bb0-852c-408a-92dc-764a09a18af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-07-10 03:32:46] Created fine-tune: ft-FWcyueisQGBddHjFgZMBtr5C\n",
            "[2023-07-10 05:21:44] Fine-tune costs $0.02\n",
            "[2023-07-10 05:21:44] Fine-tune enqueued. Queue number: 4\n",
            "[2023-07-10 05:24:34] Fine-tune is in the queue. Queue number: 3\n",
            "[2023-07-10 05:24:40] Fine-tune is in the queue. Queue number: 2\n",
            "[2023-07-10 05:24:42] Fine-tune is in the queue. Queue number: 1\n",
            "[2023-07-10 05:24:52] Fine-tune started\n",
            "[2023-07-10 05:26:05] Completed epoch 1/4\n",
            "[2023-07-10 05:26:19] Completed epoch 2/4\n",
            "[2023-07-10 05:26:31] Completed epoch 3/4\n",
            "[2023-07-10 05:26:43] Completed epoch 4/4\n",
            "[2023-07-10 05:27:05] Uploaded model: curie:ft-personal-2023-07-10-05-27-04\n",
            "[2023-07-10 05:27:06] Uploaded result file: file-wAz5UzjfNKjduuzV87cfJLzk\n",
            "[2023-07-10 05:27:06] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded ðŸŽ‰\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m curie:ft-personal-2023-07-10-05-27-04 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#openai api fine_tunes.follow -i ft-FWcyueisQGBddHjFgZMBtr5C\n",
        "!openai api fine_tunes.results -i ft-FWcyueisQGBddHjFgZMBtr5C > result.csv\n",
        "results = pd.read_csv('result.csv')\n",
        "results[results['classification/accuracy'].notnull()].tail(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "6E766jAb25Li",
        "outputId": "c48146aa-b3f4-40ba-edc7-1332fcdd68cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     step  elapsed_tokens  elapsed_examples  training_loss  \\\n",
              "276   277            8085               277       0.006622   \n",
              "\n",
              "     training_sequence_accuracy  training_token_accuracy  validation_loss  \\\n",
              "276                         1.0                      1.0              NaN   \n",
              "\n",
              "     validation_sequence_accuracy  validation_token_accuracy  \\\n",
              "276                           NaN                        NaN   \n",
              "\n",
              "     classification/accuracy  classification/weighted_f1_score  \n",
              "276                      1.0                               1.0  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-3736b67a-5a2d-4d82-9e3b-7e8606dc1f11\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>step</th>\n",
              "      <th>elapsed_tokens</th>\n",
              "      <th>elapsed_examples</th>\n",
              "      <th>training_loss</th>\n",
              "      <th>training_sequence_accuracy</th>\n",
              "      <th>training_token_accuracy</th>\n",
              "      <th>validation_loss</th>\n",
              "      <th>validation_sequence_accuracy</th>\n",
              "      <th>validation_token_accuracy</th>\n",
              "      <th>classification/accuracy</th>\n",
              "      <th>classification/weighted_f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>277</td>\n",
              "      <td>8085</td>\n",
              "      <td>277</td>\n",
              "      <td>0.006622</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3736b67a-5a2d-4d82-9e3b-7e8606dc1f11')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-d69bef9f-6a32-4bfb-9049-e2d3c5f3ca33\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d69bef9f-6a32-4bfb-9049-e2d3c5f3ca33')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-d69bef9f-6a32-4bfb-9049-e2d3c5f3ca33 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3736b67a-5a2d-4d82-9e3b-7e8606dc1f11 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3736b67a-5a2d-4d82-9e3b-7e8606dc1f11');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the new fine-tuned model\n"
      ],
      "metadata": {
        "id": "55cmgSQrCb9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api completions.create -m curie:ft-personal-2023-07-10-05-27-04 -p \"This csv lists text about pieces of equipment and then classifies them as refering to a broken item or other engagement:\\n\\n Bo went out to meet bojangles, other\\nthe wing fell off,\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpjHaijNDEes",
        "outputId": "c8af3d04-e586-4c3d-ec6a-aba52954ea02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This csv lists text about pieces of equipment and then classifies them as refering to a broken item or other engagement:\\n\\n Bo went out to meet bojangles, other\\nthe wing fell off, still functional. Requesting evaluation -> Broken -> Broken -> Broken -> Broken -> Broken"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.Completion.create(\n",
        "    model=\"curie:ft-personal-2023-07-10-05-27-04\",\n",
        "    prompt=\"bojangles dropped the glass; it was unharmed by the unit\")"
      ],
      "metadata": {
        "id": "cjjQtVkx5UYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e96b887a-ec9b-4d79-cc52-491fa6c0bd67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7ayfYhPPERHFENAaslGDLX0OvR9Sp at 0x7ff8d3e90e50> JSON: {\n",
              "  \"id\": \"cmpl-7ayfYhPPERHFENAaslGDLX0OvR9Sp\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"created\": 1689047376,\n",
              "  \"model\": \"curie:ft-personal-2023-07-10-05-27-04\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"text\": \"'s protection mechanism -> Broken -> Broken -> Broken -> Broken -> Broken -> Broken ->\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"finish_reason\": \"length\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 15,\n",
              "    \"completion_tokens\": 16,\n",
              "    \"total_tokens\": 31\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can we do better?\n",
        "\n",
        "openai.Completion.create(\n",
        "  model=\"curie:ft-personal-2023-07-10-05-27-04\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a text classification bot that responds with one-word answers.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Classify this text as related to a 'broken' item, 'training' exercise, or 'other': 'The wing broke off my drone.'\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Broken\"},\n",
        "        {\"role\": \"user\", \"content\": \"The bird flew sucessfuly\"}]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "a7Egl-fa48SC",
        "outputId": "5f6ff2c3-893c-428f-bc0e-8c6899e43faa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3cbb1f7e3cd5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Can we do better?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m openai.Completion.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"curie:ft-personal-2023-07-10-05-27-04\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: Unrecognized request argument supplied: messages"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uh-oh!\n",
        "Looks like we cant use our handy role + chat sequence with fine tuned models that easily. We have to choose between the two.   \n",
        "**To-do**:\n",
        " - Try to apply a \"role\" to our fine-tuned model\n",
        " - Compare gtp-3.5 to our fine tuned model\n",
        " - Improve our training data so this model works"
      ],
      "metadata": {
        "id": "cjGbXzP-69d8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aifl6gHa7y8O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}